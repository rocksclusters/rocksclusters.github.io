<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2008 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>3.4 User Priorities and Negotiation</TITLE>
<META NAME="description" CONTENT="3.4 User Priorities and Negotiation">
<META NAME="keywords" CONTENT="ref">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2008">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="ref.css">

<LINK REL="next" HREF="3_5Policy_Configuration.html">
<LINK REL="previous" HREF="3_3Configuration.html">
<LINK REL="up" HREF="3_Administrators_Manual.html">
<LINK REL="next" HREF="3_5Policy_Configuration.html">
</HEAD>

<BODY  BGCOLOR=#FFFFFF >
<!--Navigation Panel-->
<A NAME="tex2html1493"
  HREF="3_5Policy_Configuration.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next" SRC="next.png"></A> 
<A NAME="tex2html1487"
  HREF="3_Administrators_Manual.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up.png"></A> 
<A NAME="tex2html1481"
  HREF="3_3Configuration.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="prev.png"></A> 
<A NAME="tex2html1489"
  HREF="Contents.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents" SRC="contents.png"></A> 
<A NAME="tex2html1491"
  HREF="Index.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index" SRC="index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html1494"
  HREF="3_5Policy_Configuration.html">3.5 Policy Configuration for</A>
<B> Up:</B> <A NAME="tex2html1488"
  HREF="3_Administrators_Manual.html">3. Administrators' Manual</A>
<B> Previous:</B> <A NAME="tex2html1482"
  HREF="3_3Configuration.html">3.3 Configuration</A>
 &nbsp; <B>  <A NAME="tex2html1490"
  HREF="Contents.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html1492"
  HREF="Index.html">Index</A></B> 
<BR>
<BR>
<!--End of Navigation Panel-->
<!--Table of Child-Links-->
<A NAME="CHILD_LINKS"><STRONG>Subsections</STRONG></A>

<UL>
<LI><A NAME="tex2html1495"
  HREF="3_4User_Priorities.html#SECTION00441000000000000000">3.4.1 Real User Priority (RUP)</A>
<LI><A NAME="tex2html1496"
  HREF="3_4User_Priorities.html#SECTION00442000000000000000">3.4.2 Effective User Priority (EUP)</A>
<LI><A NAME="tex2html1497"
  HREF="3_4User_Priorities.html#SECTION00443000000000000000">3.4.3 Priorities in Negotiation and Preemption</A>
<LI><A NAME="tex2html1498"
  HREF="3_4User_Priorities.html#SECTION00444000000000000000">3.4.4 Priority Calculation</A>
<LI><A NAME="tex2html1499"
  HREF="3_4User_Priorities.html#SECTION00445000000000000000">3.4.5 Negotiation</A>
<LI><A NAME="tex2html1500"
  HREF="3_4User_Priorities.html#SECTION00446000000000000000">3.4.6 The Layperson's Description of the Pie Spin and Pie Slice</A>
<LI><A NAME="tex2html1501"
  HREF="3_4User_Priorities.html#SECTION00447000000000000000">3.4.7 Group Accounting</A>
<LI><A NAME="tex2html1502"
  HREF="3_4User_Priorities.html#SECTION00448000000000000000">3.4.8 Accounting Groups with Hierarchical Group Quotas</A>
</UL>
<!--End of Table of Child-Links-->
<HR>

<H1><A NAME="SECTION00440000000000000000"></A><A NAME="sec:UserPrio"></A>
<BR>
3.4 User Priorities and Negotiation
</H1>

<P>
<A NAME="30889"></A>
<A NAME="30890"></A>
HTCondor uses priorities to determine machine allocation for jobs.
This section details the priorities and the allocation of
machines (negotiation).

<P>
For accounting purposes, each user is identified by username@uid_domain.
Each user is assigned a priority value even if submitting jobs from
different machines in the same domain, or even if submitting from multiple
machines in the different domains.

<P>
The numerical priority value assigned to a user is inversely related to the 
<I>goodness</I> of the priority.
A user with a numerical priority of 5 gets 
more resources than a user with a numerical priority of 50.
There are two 
priority values assigned to HTCondor users:

<UL>
<LI>Real User Priority (RUP), which measures resource usage of the 
		user.
</LI>
<LI>Effective User Priority (EUP), which determines the number of
		resources the user can get.
</LI>
</UL>
This section describes these two priorities and how they affect resource
allocations in HTCondor.
Documentation on configuring and controlling 
priorities may be found in section&nbsp;<A HREF="3_3Configuration.html#sec:Negotiator-Config-File-Entries">3.3.17</A>.

<P>

<H2><A NAME="SECTION00441000000000000000"></A><A NAME="sec:RUP"></A>
<A NAME="30896"></A>
<A NAME="30897"></A>
<BR>
3.4.1 Real User Priority (RUP)
</H2>
A user's RUP measures the resource usage of the user 
through time.
Every user begins with a RUP of one half (0.5), and
at steady state, the RUP of a user equilibrates to the number of resources 
used by that user.  Therefore, if a specific user continuously uses exactly 
ten resources for a long period of time, the RUP of that user stabilizes at 
ten.

<P>
However, if the user decreases the number of resources used, the RUP
gets better.  The rate at which the priority value decays 
can be set by the macro <TT>PRIORITY_HALFLIFE</TT><A NAME="31119"></A><A NAME="31120"></A>, a time period 
defined in seconds.   Intuitively, if the <TT>PRIORITY_HALFLIFE</TT><A NAME="31124"></A><A NAME="31125"></A> in a pool 
is set to 86400 (one day), and if a user whose RUP was 10 has no
running jobs, 
that user's RUP would be 5 one day later, 2.5 two days later,
and so on.

<P>

<H2><A NAME="SECTION00442000000000000000"></A><A NAME="sec:EUP"></A>
<A NAME="30901"></A>
<A NAME="30902"></A>
<BR>
3.4.2 Effective User Priority (EUP)
</H2>
The effective user priority (EUP) of a user is used to determine
how many resources that user may receive.
The EUP is linearly related to the RUP
by a <I>priority factor</I> which may be defined on a per-user basis.
Unless otherwise configured, 
an initial priority factor for all users as they first submit jobs
is set by the configuration variable <TT>DEFAULT_PRIO_FACTOR</TT><A NAME="31129"></A><A NAME="31130"></A>,
and defaults to the value 1000.0.
If desired, the priority factors of
specific users can be increased using <I>condor_userprio</I>,
so that some are served preferentially.

<P>
The number of resources that a user may receive is inversely related
to the ratio between the EUPs of submitting users.
Therefore user <IMG
 WIDTH="19" HEIGHT="16" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$A$"> with EUP=5 will receive
twice as many resources as user <IMG
 WIDTH="20" HEIGHT="16" ALIGN="BOTTOM" BORDER="0"
 SRC="img43.png"
 ALT="$B$"> with EUP=10 and four times as many 
resources as user <IMG
 WIDTH="19" HEIGHT="16" ALIGN="BOTTOM" BORDER="0"
 SRC="img44.png"
 ALT="$C$"> with EUP=20.
However, if <IMG
 WIDTH="19" HEIGHT="16" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$A$"> does not use the full number
of resources that <IMG
 WIDTH="19" HEIGHT="16" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.png"
 ALT="$A$"> may be given,
the available resources are repartitioned and distributed among
remaining users according to the inverse ratio rule.

<P>
HTCondor supplies mechanisms to directly support two policies in which EUP may
be useful:
<DL>
<DT><STRONG>Nice users</STRONG></DT>
<DD>A job may be submitted with the submit command 
	<B>nice_user</B><A NAME="31136"></A> set to <TT>True</TT>.
	This nice user job will have its RUP boosted by the 
	<TT>NICE_USER_PRIO_FACTOR</TT><A NAME="31139"></A><A NAME="31140"></A> priority factor specified in the 
	configuration, leading to a very large EUP.
	This corresponds to a low priority for resources,
	therefore using resources not used by other HTCondor users.

<P>
</DD>
<DT><STRONG>Remote Users</STRONG></DT>
<DD>HTCondor's flocking feature (see
	section&nbsp;<A HREF="5_2Connecting_HTCondor.html#sec:Flocking">5.2</A>) allows jobs to run in a pool
        other than the local one.
	In addition, the submit-only feature allows a user 
	to submit jobs to another pool.
	In such situations, submitters from other domains
	can submit to the local pool.
	It may be desirable to have HTCondor treat local users
	preferentially over these remote users.
	If configured, HTCondor will boost the RUPs of remote users by
	<TT>REMOTE_PRIO_FACTOR</TT><A NAME="31144"></A><A NAME="31145"></A>
	specified in the configuration,
	thereby lowering their priority for resources.
</DD>
</DL>

<P>
The priority boost factors for individual users can be set with the 
<B>setfactor</B> option of <I>condor_userprio</I>.
Details may be found in the <I>condor_userprio</I> manual page 
on page&nbsp;<A HREF="condor_userprio.html#man-condor-userprio"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]" SRC="crossref.png"></A>.

<P>

<H2><A NAME="SECTION00443000000000000000"></A><A NAME="sec:Priorities-in-Negotiation-and-Preemption"></A>
<A NAME="30918"></A>
<A NAME="30919"></A>
<A NAME="30920"></A>
<BR>
3.4.3 Priorities in Negotiation and Preemption
</H2>
Priorities are used to ensure that users get their fair share of resources.  
The priority values are used at allocation time, meaning during
negotiation and matchmaking.
Therefore, there are ClassAd attributes that take on defined values
only during negotiation, making them ephemeral.
In addition to allocation, HTCondor may preempt a machine claim 
and reallocate it when conditions change.

<P>
Too many preemptions lead to thrashing,
a condition in which negotiation for a machine identifies a
new job with a better priority most every cycle.
Each job is, in turn, preempted, and no job finishes.
To avoid this situation, the <TT>PREEMPTION_REQUIREMENTS</TT><A NAME="31155"></A><A NAME="31156"></A> 
configuration variable is defined for and used only by 
the <I>condor_negotiator</I> daemon
to specify the conditions that must be met for a preemption to
occur.  When preemption is enabled,
it is usually defined to deny preemption if a current running
job has been running for a relatively short period of time.  This
effectively limits the number of preemptions per resource per time
interval.
Note that <TT>PREEMPTION_REQUIREMENTS</TT> only applies to preemptions
due to user priority.  It does not have any effect if the machine's 
<TT>RANK</TT>
expression prefers a different job, or if the machine's policy
causes the job to vacate due to other activity on the machine.
See section <A HREF="3_5Policy_Configuration.html#sec:DisablingPreemption">3.5.9</A> for the current default
policy on preemption.

<P>
The following ephemeral attributes may be used within policy definitions.
Care should be taken when using these attributes, 
due to their ephemeral nature;
they are not always defined, so the usage of an expression to 
check if defined such as
<PRE>
  (RemoteUserPrio =?= UNDEFINED)
</PRE>
is likely necessary.

<P>
Within these attributes, those with names that contain the
string <TT>Submitter</TT> refer to characteristics about the candidate job's user;
those with names that contain the string <TT>Remote</TT>
refer to characteristics about the user currently using the resource.
Further,  those with names that end with the
string <TT>ResourcesInUse</TT> have values that may change within
the time period associated with a single negotiation cycle.
Therefore, the configuration variables <TT>PREEMPTION_REQUIREMENTS_STABLE</TT><A NAME="31167"></A><A NAME="31168"></A>
and and <TT>PREEMPTION_RANK_STABLE</TT><A NAME="31172"></A><A NAME="31173"></A> exist to inform the 
<I>condor_negotiator</I> daemon that values may change.
See section&nbsp;<A HREF="3_3Configuration.html#param:PreemptionRequirementsStable">3.3.17</A> on
page&nbsp;<A HREF="3_3Configuration.html#param:PreemptionRequirementsStable"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]" SRC="crossref.png"></A> for
definitions of these configuration variables.

<P>
<DL>
<DT><STRONG><A NAME="30937"></A><TT>SubmitterUserPrio</TT>:</STRONG></DT>
<DD>A floating point value representing the user priority of the 
  candidate job.
</DD>
<DT><STRONG><A NAME="30939"></A><TT>SubmitterUserResourcesInUse</TT>:</STRONG></DT>
<DD>The integer number of slots currently utilized by the user
  submitting the candidate job.
</DD>
<DT><STRONG><A NAME="30941"></A><TT>RemoteUserPrio</TT>:</STRONG></DT>
<DD>A floating point value representing the user priority of the 
  job currently running on the machine.
  This version of the attribute, with no slot represented in the attribute name,
  refers to the current slot being evaluated.
</DD>
<DT><STRONG><A NAME="30943"></A><TT>Slot&lt;N&gt;_RemoteUserPrio</TT>:</STRONG></DT>
<DD>A floating point value representing the user priority of the 
  job currently running on the particular slot represented
  by <code>&lt;N&gt;</code> on the machine.
</DD>
<DT><STRONG><A NAME="30945"></A><TT>RemoteUserResourcesInUse</TT>:</STRONG></DT>
<DD>The integer number of slots currently utilized by the user
  of the job currently running on the machine.
</DD>
<DT><STRONG><A NAME="30947"></A><TT>SubmitterGroupResourcesInUse</TT>:</STRONG></DT>
<DD>If the owner of the candidate job is a member of a valid accounting group,
  with a defined group quota,
  then this attribute is the integer number of slots currently 
  utilized by the group.
</DD>
<DT><STRONG><A NAME="30949"></A><TT>SubmitterGroup</TT>:</STRONG></DT>
<DD>The accounting group name of the requesting submitter.
</DD>
<DT><STRONG><A NAME="30951"></A><TT>SubmitterGroupQuota</TT>:</STRONG></DT>
<DD>If the owner of the candidate job is a member of a valid accounting group,
  with a defined group quota,
  then this attribute is the integer number of slots defined as the
  group's quota.
</DD>
<DT><STRONG><A NAME="30953"></A><TT>RemoteGroupResourcesInUse</TT>:</STRONG></DT>
<DD>If the owner of the currently running job is a member of a valid 
  accounting group, with a defined group quota,
  then this attribute is the integer number of slots currently 
  utilized by the group.
</DD>
<DT><STRONG><A NAME="30955"></A><TT>RemoteGroup</TT>:</STRONG></DT>
<DD>The accounting group name of the owner of the currently running job.
</DD>
<DT><STRONG><A NAME="30957"></A><TT>RemoteGroupQuota</TT>:</STRONG></DT>
<DD>If the owner of the currently running job is a member of a valid 
  accounting group, with a defined group quota,
  then this attribute is the integer number of slots defined as the
  group's quota.
</DD>
<DT><STRONG><A NAME="30959"></A><TT>SubmitterNegotiatingGroup</TT>:</STRONG></DT>
<DD>The accounting group name that the candidate job is negotiating under.
</DD>
<DT><STRONG><A NAME="30961"></A><TT>RemoteNegotiatingGroup</TT>:</STRONG></DT>
<DD>The accounting group name that the currently running job negotiated under.
</DD>
<DT><STRONG><A NAME="30963"></A><TT>SubmitterAutoregroup</TT>:</STRONG></DT>
<DD>Boolean attribute is <TT>True</TT> if candidate job is negotiated via autoregoup.
</DD>
<DT><STRONG><A NAME="30966"></A><TT>RemoteAutoregroup</TT>:</STRONG></DT>
<DD>Boolean attribute is <TT>True</TT> if currently running job negotiated via autoregoup.
</DD>
</DL>

<P>

<H2><A NAME="SECTION00444000000000000000">
3.4.4 Priority Calculation</A>
</H2>
This section may be skipped if the reader so feels, but for the curious,
here is HTCondor's priority calculation algorithm.

<P>
The RUP of a user <IMG
 WIDTH="16" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img45.png"
 ALT="$u$"> at time <IMG
 WIDTH="12" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img16.png"
 ALT="$t$">, <IMG
 WIDTH="62" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img46.png"
 ALT="$\pi_r(u,t)$">, is calculated 
every time interval <IMG
 WIDTH="20" HEIGHT="16" ALIGN="BOTTOM" BORDER="0"
 SRC="img47.png"
 ALT="$\delta t$"> using the formula 
<BR><P></P>
<DIV ALIGN="CENTER">
<!-- MATH
 \begin{displaymath}
\pi_r(u,t) = \beta\times\pi(u,t-\delta t) + (1-\beta)\times\rho(u,t)
\end{displaymath}
 -->

<IMG
 WIDTH="347" HEIGHT="32" BORDER="0"
 SRC="img48.png"
 ALT="\begin{displaymath}\pi_r(u,t) = \beta\times\pi(u,t-\delta t) + (1-\beta)\times\rho(u,t)\end{displaymath}">
</DIV>
<BR CLEAR="ALL">
<P></P>
where <IMG
 WIDTH="53" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img49.png"
 ALT="$\rho(u,t)$"> is the number of resources used by user <IMG
 WIDTH="16" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img45.png"
 ALT="$u$"> at time <IMG
 WIDTH="12" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img16.png"
 ALT="$t$">,
and <!-- MATH
 $\beta=0.5^{{\delta t}/h}$
 -->
<IMG
 WIDTH="93" HEIGHT="41" ALIGN="MIDDLE" BORDER="0"
 SRC="img50.png"
 ALT="$\beta=0.5^{{\delta t}/h}$">. <IMG
 WIDTH="16" HEIGHT="16" ALIGN="BOTTOM" BORDER="0"
 SRC="img51.png"
 ALT="$h$"> is the half life period set by 
<TT>PRIORITY_HALFLIFE</TT><A NAME="31196"></A><A NAME="31197"></A>.

<P>
The EUP of user <IMG
 WIDTH="16" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img45.png"
 ALT="$u$"> at time <IMG
 WIDTH="12" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img16.png"
 ALT="$t$">, <IMG
 WIDTH="62" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img52.png"
 ALT="$\pi_e(u,t)$">
is calculated by
<BR><P></P>
<DIV ALIGN="CENTER">
<!-- MATH
 \begin{displaymath}
\pi_e(u,t) = \pi_r(u,t)\times f(u,t)
\end{displaymath}
 -->

<IMG
 WIDTH="207" HEIGHT="32" BORDER="0"
 SRC="img53.png"
 ALT="\begin{displaymath}\pi_e(u,t) = \pi_r(u,t)\times f(u,t)\end{displaymath}">
</DIV>
<BR CLEAR="ALL">
<P></P>
where <IMG
 WIDTH="55" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img54.png"
 ALT="$f(u,t)$"> is the priority boost factor for user <IMG
 WIDTH="16" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img45.png"
 ALT="$u$"> at time <IMG
 WIDTH="12" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img16.png"
 ALT="$t$">.

<P>
As mentioned previously, the RUP calculation is designed so that at steady
state, each user's RUP stabilizes at the number of resources used by that user. 
The definition of <IMG
 WIDTH="16" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img55.png"
 ALT="$\beta$"> ensures that the calculation of <IMG
 WIDTH="62" HEIGHT="36" ALIGN="MIDDLE" BORDER="0"
 SRC="img46.png"
 ALT="$\pi_r(u,t)$"> can be 
calculated over non-uniform time intervals <IMG
 WIDTH="20" HEIGHT="16" ALIGN="BOTTOM" BORDER="0"
 SRC="img47.png"
 ALT="$\delta t$"> without affecting the 
calculation.  The time interval <IMG
 WIDTH="20" HEIGHT="16" ALIGN="BOTTOM" BORDER="0"
 SRC="img47.png"
 ALT="$\delta t$"> varies due to events internal to 
the system, but HTCondor guarantees that unless the central manager machine is 
down, no matches will be unaccounted for due to this variance.

<P>

<H2><A NAME="SECTION00445000000000000000"></A><A NAME="sec:negotiation"></A>
<A NAME="30974"></A>
<A NAME="30975"></A>
<BR>
3.4.5 Negotiation
</H2>

<P>
Negotiation is the method HTCondor undergoes periodically to
match queued jobs with resources capable of running jobs.
The <I>condor_negotiator</I> daemon is responsible for
negotiation.

<P>
During a negotiation cycle, the <I>condor_negotiator</I> daemon
accomplishes the following ordered list of items.

<OL>
<LI>Build a list of all possible resources,
  regardless of the state of those resources.
</LI>
<LI>Obtain a list of all job submitters (for the entire pool).
</LI>
<LI>Sort the list of all job submitters based on EUP
  (see section&nbsp;<A HREF="#sec:EUP">3.4.2</A> for an explanation of EUP).
  The submitter with the best priority is first within the sorted list.
</LI>
<LI>Iterate until there are either no more resources to match,
  or no more jobs to match.
    <DL>
<DT></DT>
<DD>For each submitter (in EUP order):
        <DL>
<DT></DT>
<DD>For each submitter, get each job.
	  Since jobs may be submitted from more than one machine
	  (hence to more than one <I>condor_schedd</I> daemon),
	  here is a further definition of the ordering of these jobs.
	  With jobs from a single <I>condor_schedd</I> daemon, 
	  jobs are typically returned in job priority order.
	  When more than one <I>condor_schedd</I> daemon is involved,
	  they are contacted in an undefined order. 
	  All jobs from a single <I>condor_schedd</I> daemon are considered
	  before moving on to the next.
	  For each job:
              
<UL>
<LI>For each machine in the pool that can execute jobs:
                  
<OL>
<LI>If <TT>machine.requirements</TT> evaluates to
		  <TT>False</TT> or
		  <TT>job.requirements</TT> evaluates to <TT>False</TT>,
		  skip this machine
</LI>
<LI>If the machine is in the Claimed state, but
		  not running a job, skip this machine.
</LI>
<LI>If this machine is not running a job, add it to
		  the potential match list by reason of
		  No Preemption.
</LI>
<LI>If the machine is running a job
                    
<UL>
<LI>If the <TT>machine.RANK</TT> on this job is
		    better than the running job, add this machine to the
		    potential match list by reason of Rank.
</LI>
<LI>If the EUP of this job is better than the EUP
		    of the currently running job, and
		    <TT>PREEMPTION_REQUIREMENTS</TT> is <TT>True</TT>,
		    and the <TT>machine.RANK</TT> on this job is not worse
		    than the currently running job, add this machine to the
		    potential match list by reason of Priority.
                    
</LI>
</UL>
</LI>
</OL>
</LI>
<LI>Of machines in the potential match list, sort by
		<TT>NEGOTIATOR_PRE_JOB_RANK</TT>, <TT>job.RANK</TT>,
		<TT>NEGOTIATOR_POST_JOB_RANK</TT>, Reason for claim
		(No Preemption, then Rank, then Priority),
		<TT>PREEMPTION_RANK</TT>
</LI>
<LI>The job is assigned to the top machine on the
		potential match list.  The machine is removed
		from the list of resources to match
		(on this negotiation cycle).
              
</LI>
</UL>
</DD>
</DL>
</DD>
</DL>
</LI>
</OL>

<P>
The <I>condor_negotiator</I> asks the <I>condor_schedd</I> for the 
"next job" from a given submitter/user.
Typically, the <I>condor_schedd</I> returns jobs in the order of
job priority.
If priorities are the same,
job submission time is used; older jobs go first.
If a cluster has multiple procs in it and one of the jobs cannot be matched,
the <I>condor_schedd</I> will not return any more jobs in that cluster
on that negotiation pass.
This is an optimization based on the theory that the cluster jobs are similar.
The configuration variable <TT>NEGOTIATE_ALL_JOBS_IN_CLUSTER</TT><A NAME="31233"></A><A NAME="31234"></A>
disables the cluster-skipping optimization.
Use of the configuration variable <TT>SIGNIFICANT_ATTRIBUTES</TT><A NAME="31238"></A><A NAME="31239"></A>
will change the
definition of what the <I>condor_schedd</I> considers a cluster from the
default definition of all jobs that share the same <TT>ClusterId</TT>.

<P>

<H2><A NAME="SECTION00446000000000000000"></A><A NAME="sec:PieSlice"></A>
<A NAME="31016"></A>
<A NAME="31017"></A>
<A NAME="31018"></A>
<A NAME="31019"></A>
<BR>
3.4.6 The Layperson's Description of the Pie Spin and Pie Slice
</H2>

<P>
HTCondor schedules in a variety of ways.
First, it takes all users who have submitted jobs and calculates their priority.
Then, it totals the number of resources available at the moment,
and using the ratios of the user priorities,
it calculates the number of machines each user could get.
This is their <I>pie slice</I>.

<P>
The HTCondor matchmaker goes in user priority order, 
contacts each user, and asks for job information. 
The <I>condor_schedd</I> daemon (on behalf of a user)
tells the matchmaker about a job,
and the matchmaker looks at available resources
to create a list of resources that match the requirements expression.
With the list of resources that match,
it sorts them according to the rank expressions within ClassAds.
If a machine prefers a job, the job is assigned to that machine,
potentially preempting a job that might already be running on that machine.
Otherwise, give the machine to the job that the job ranks highest.
If the machine ranked highest is already running a job,
we may preempt running job
for the new job. 
When preemption is enabled,
a reasonable policy states that the user must
have a 20%  better priority in order for preemption to succeed.
If the job has no preferences as to what sort of machine it gets,
matchmaking gives it the first idle resource to meet its requirements.

<P>
This matchmaking cycle continues until the user has received all of the
machines in their pie slice.
The matchmaker then contacts the next highest
priority user and offers that user their pie slice worth of machines.
After contacting all users,
the cycle is repeated with any still available resources
and recomputed pie slices.
The matchmaker continues <I>spinning the pie</I> 
until it runs out of machines or all the <I>condor_schedd</I> daemons
say they have no more jobs. 

<P>

<H2><A NAME="SECTION00447000000000000000"></A><A NAME="sec:group-accounting"></A>
<A NAME="31025"></A>
<A NAME="31026"></A>
<A NAME="31027"></A>
<BR>
3.4.7 Group Accounting
</H2>

<P>
By default, HTCondor does all accounting on a per-user basis, and this
accounting is primarily used to compute priorities for HTCondor's
fair-share scheduling algorithms. 
However, accounting can also be done on a per-group basis.
Multiple users can all submit jobs into the same accounting group,
and all jobs with the same accounting group
will be treated with the same priority.
Jobs that do <I>not</I> specify an accounting group have 
all accounting and priority based on the user, 
which may be identified by the job ClassAd attribute <TT>Owner</TT>.
Jobs that do specify an accounting group have 
all accounting and priority based on the specified accounting group.
Therefore, accounting based on groups only works when
the jobs correctly identify their group membership.

<P>
<A NAME="31030"></A>
<A NAME="31031"></A>
The preferred method for having a job associate itself with
an accounting group adds a command to the submit description file
that specifies the group name:
<PRE>
  accounting_group = group_physics
</PRE>
This command causes the job ClassAd attribute <TT>AcctGroup</TT> to
be set with this group name.

<P>
If the user name of the job submitter should be other than the <TT>Owner</TT>
job ClassAd attribute, an additional command specifies the user name:
<PRE>
  accounting_group_user = albert
</PRE>
This command causes the job ClassAd attribute <TT>AcctGroupUser</TT> to
be set with this user name.

<P>
<A NAME="31039"></A>
The previous method for defining accounting groups
is no longer recommended. 
It inserted the job ClassAd attribute <TT>AccountingGroup</TT>
by setting it in the submit description file using the syntax 
in this example:
<PRE>
+AccountingGroup = "group_physics.albert"
</PRE>

<P>
In this previous method for defining accounting groups,
the <TT>AccountingGroup</TT> attribute is a string,
and it therefore must be enclosed in double quote marks.

<P>
Much of the reason that the previous method for defining accounting groups
is no longer recommended  is that the name of an accounting is that it
used the period (.) character to separate the group name from the
user name.  Therefore, the syntax did not work if a user name contained
a period.

<P>
The name should <I>not</I> be qualified with a domain.
Certain parts of the HTCondor system 
do append the value <TT>$(UID_DOMAIN)</TT>
(as specified in the configuration file on the submit machine)
to this string for internal use.
For example, if the value of <TT>UID_DOMAIN</TT> is
<TT>example.com</TT>, and the accounting group name
is as specified,
<I>condor_userprio</I> will show statistics
for this accounting group using the appended domain, for example
<PRE>
                                    Effective
User Name                           Priority
------------------------------      ---------
group_physics@example.com                0.50
user@example.com                        23.11
heavyuser@example.com                  111.13
...
</PRE>
<P>
Additionally, the <I>condor_userprio</I> command allows administrators to
remove an entity from the accounting system in HTCondor.
The <B>-delete</B> option to <I>condor_userprio</I>
accomplishes this
if all the jobs from a given accounting group are completed,
and the administrator wishes to remove that group from the system.
The <B>-delete</B> option
identifies the accounting group with the fully-qualified name
of the accounting group.
For example
<PRE>
condor_userprio -delete group_physics@example.com
</PRE>
<P>
HTCondor removes entities itself as they are no longer
relevant.
Intervention by an administrator to delete entities can
be beneficial when the use of thousands
of short term accounting groups leads to scalability
issues.

<P>

<H2><A NAME="SECTION00448000000000000000"></A><A NAME="sec:group-quotas"></A>
<BR>
3.4.8 Accounting Groups with Hierarchical Group Quotas
</H2>

<P>
<A NAME="31058"></A>
<A NAME="31059"></A>
<A NAME="31060"></A>
<A NAME="31061"></A>
An upper limit on the number of slots allocated to a group
of users can be specified with group quotas.  
This policy may be desired when different groups provide their computers
to create one large HTCondor pool, and want to restrict the number
of jobs running from one group to the number of machines the group
has provided.

<P>
Consider an example pool with thirty slots:
twenty slots are owned by the physics group and ten
are owned by the chemistry group.  The desired
policy is that no more than twenty concurrent jobs are ever running
from the physicists, and only ten from the chemists.
These machines are otherwise identical, so it does not matter
which machines run which group's jobs.
It only matters that the proportions of allocated slots are correct.

<P>
Instead of quotas, this could be implemented by configuring 
the <TT>RANK</TT> expression such that the twenty machines owned by 
the physics group prefer jobs submitted by the physics users.
Likewise, the ten machines owned by the chemistry group are configured 
to prefer jobs submitted by the chemistry group. 
However, this steers jobs to execute on specific machines, 
instead of the desired policy which allocates numbers of machines,
where these machines can be any of the pool's machines that are available.

<P>
Group quotas may implement this policy.
Define the groups and set their quotas in the 
configuration of the central manager:

<P>
<PRE>
  GROUP_NAMES = group_physics, group_chemistry
  GROUP_QUOTA_group_physics =   20
  GROUP_QUOTA_group_chemistry = 10
</PRE>

<P>
The implementation of quotas is hierarchical, 
such that quotas may be described for the tree of groups, subgroups, 
sub subgroups, etc.
Group names identify the groups,  
such that the configuration can define the quotas in terms of 
limiting the number of cores allocated for a group or subgroup. 
Group names do not need to begin with <TT>"group_"</TT>, 
but that is the convention, 
which helps to avoid naming conflicts between groups and subgroups.  
The hierarchy is identified by using the period ('.') character to 
separate a group name from a subgroup name from a sub subgroup name, etc.
Group names are case-insensitive for negotiation.

<P>
<A NAME="31066"></A>
<A NAME="31067"></A>
At the root of the tree that defines the hierarchical groups is the
invented "&lt;none&gt;" group. 
The implied quota of the "&lt;none&gt;" group will be all available slots.
This string will appear in the output of <I>condor_status</I>. 

<P>
If the sum of the child quotas exceeds the parent, 
then the child quotas are scaled down in proportion to their relative sizes.
For the given example, 
there were 30 original slots at the root of the tree.
If a power failure removed half of the original 30,
leaving fifteen slots,
physics would be scaled back to a quota of ten, 
and chemistry to five.  
This scaling can be disabled by setting the <I>condor_negotiator</I>
configuration variable <TT>NEGOTIATOR_ALLOW_QUOTA_OVERSUBSCRIPTION</TT><A NAME="31277"></A><A NAME="31278"></A>
to <TT>True</TT>.
If the sum of the child quotas is less than that of the parent, 
the child quotas remain intact; they are not scaled up.
That is, if somehow the number of slots doubled from thirty to sixty, 
physics would still be limited to 20 slots, 
and chemistry would be limited to 10.
This example in which the quota is defined by absolute
values is called a static quota.

<P>
Each job must state which group it belongs to.  
Currently this is opt-in, 
and the system trusts each user to put the correct group 
in the submit description file.
Jobs that do not identify themselves as a group member are 
negotiated for as part of the  "&lt;none&gt;" group.
Note that this requirement is per job, not per user.  
A given user may be a member of many groups.
Jobs identify which group they are in by setting the 
<B>accounting_group</B><A NAME="31283"></A>
and <B>accounting_group_user</B><A NAME="31285"></A> commands within
the submit description file, 
as specified in section&nbsp;<A HREF="#sec:group-accounting">3.4.7</A>.
For example:

<P>
<PRE>
accounting_group = group_physics
accounting_group_user = einstein
</PRE>

<P>
The size of the quotas may instead be expressed as a proportion.
This is then referred to as a dynamic group quota, 
because the size of the quota is dynamically recalculated every
negotiation cycle, 
based on the total available size of the pool.
Instead of using static quotas,
this example can be recast using dynamic quotas, 
with one-third of the pool allocated to chemistry and two-thirds to physics.
The quotas maintain this ratio even as the size of the pool changes,
perhaps because of machine failures, 
because of the arrival of new machines within the pool,
or because of other reasons.  
The job submit description files remain the same. 
Configuration on the central manager becomes:

<P>
<PRE>
  GROUP_NAMES = group_physics, group_chemistry
  GROUP_QUOTA_DYNAMIC_group_chemistry = 0.33
  GROUP_QUOTA_DYNAMIC_group_physics =   0.66
</PRE>

<P>
The values of the quotas must be less than 1.0, indicating fractions
of the pool's machines.
As with static quota specification, 
if the sum of the children exceeds one, 
they are scaled down proportionally so that their sum does equal 1.0.
If their sum is less than one, they are not changed.

<P>
Extending this example to incorporate subgroups,
assume that the physics group consists of high-energy (hep) 
and low-energy (lep) subgroups.
The high-energy sub-group owns fifteen of the twenty physics slots, 
and the low-energy group owns the remainder.
Groups are distinguished from subgroups by an intervening 
period character (<code>.</code>) in the group's name.
Static quotas for these subgroups extend the example configuration:

<P>
<PRE>
  GROUP_NAMES = group_physics, group_physics.hep, group_physics.lep, group_chemistry
  GROUP_QUOTA_group_physics     =   20
  GROUP_QUOTA_group_physics.hep =   15
  GROUP_QUOTA_group_physics.lep =    5
  GROUP_QUOTA_group_chemistry   =   10
</PRE>
<P>
This hierarchy may be more useful when dynamic quotas are used.
Here is the example, using dynamic quotas:

<P>
<PRE>
  GROUP_NAMES = group_physics, group_physics.hep, group_physics.lep, group_chemistry
  GROUP_QUOTA_DYNAMIC_group_chemistry   =   0.33334
  GROUP_QUOTA_DYNAMIC_group_physics     =   0.66667
  GROUP_QUOTA_DYNAMIC_group_physics.hep =   0.75
  GROUP_QUOTA_DYNAMIC_group_physics.lep =   0.25
</PRE>
<P>
The fraction of a subgroup's quota is expressed with respect to
its parent group's quota.
That is, 
the high-energy physics subgroup is allocated 75% of the 66% that 
physics gets of the entire pool, however many that might be.
If there are 30 machines in the pool, that
would be the same 15 machines as specified in the static quota example.

<P>
High-energy physics users indicate which group their jobs 
should go in with the submit description file identification:

<P>
<PRE>
accounting_group = group_physics.hep
accounting_group_user = higgs
</PRE>

<P>
In all these examples so far,
the hierarchy is merely a notational convenience.
Each of the examples could be implemented with a flat structure, 
although it might be more confusing for the administrator.
Surplus is the concept that creates a true hierarchy.

<P>
If a given group or sub-group accepts surplus, 
then that given group is allowed to exceed its configured quota, 
by using the leftover, unused quota of other groups.
Surplus is disabled for all groups by default.  
Accepting surplus may be enabled for all groups by setting
<TT>GROUP_ACCEPT_SURPLUS</TT><A NAME="31287"></A><A NAME="31288"></A> to <TT>True</TT>.
Surplus may be enabled for individual groups
by setting <TT>GROUP_ACCEPT_SURPLUS_&lt;groupname&gt;</TT><A NAME="31293"></A><A NAME="31294"></A> to <TT>True</TT>.
Consider the following example:

<P>
<PRE>
  GROUP_NAMES = group_physics, group_physics.hep, group_physics.lep, group_chemistry
  GROUP_QUOTA_group_physics     =   20
  GROUP_QUOTA_group_physics.hep =   15
  GROUP_QUOTA_group_physics.lep =    5
  GROUP_QUOTA_group_chemistry   =   10
  GROUP_ACCEPT_SURPLUS = false
  GROUP_ACCEPT_SURPLUS_group_physics = false
  GROUP_ACCEPT_SURPLUS_group_physics.lep = true
  GROUP_ACCEPT_SURPLUS_group_physics.hep = true
</PRE>
<P>
This configuration is the same as above for the chemistry users.
However, <TT>GROUP_ACCEPT_SURPLUS</TT> is set to <TT>False</TT> globally, 
<TT>False</TT> for the physics parent group, 
and <TT>True</TT> for the subgroups group_physics.lep and group_physics.lep.
This means that group_physics.lep and group_physics.hep
are allowed to exceed their quota of 15 and 5, 
but their sum cannot exceed 20, for that is their parent's quota.
If the group_physics had <TT>GROUP_ACCEPT_SURPLUS</TT> set to <TT>True</TT>,
then either group_physics.lep and group_physics.hep would not 
be limited by quota.  

<P>
Surplus slots are distributed bottom-up from within the quota tree.
That is, any leaf nodes of this tree with 
excess quota will share it with any peers which accept surplus. 
Any subsequent excess will then be 
passed up to the parent node and over to all of its children, recursively.
Any node that does not accept surplus implements a hard cap on the 
number of slots that the sum of it's children use.

<P>
After the <I>condor_negotiator</I> calculates the quota assigned to each group, 
possibly adding in surplus, 
it then negotiates with the <I>condor_schedd</I> daemons in the system to 
try to match jobs to each group.
It does this one group at a time.  
By default, it goes in "starvation group order." 
That is, the group whose current usage is farthest under its quota goes first,
then the next, and so on.  
The "&lt;none&gt;" group implicitly at the root of the tree goes last.
This ordering can be replaced by defining configuration variable
<TT>GROUP_SORT_EXPR</TT><A NAME="31309"></A><A NAME="31310"></A>.
The <I>condor_negotiator</I> evaluates this ClassAd expression for each group 
ClassAd, 
sorts the group by the floating point result, 
and then negotiates with the largest positive value going first.
Useful attributes to use are documented
in section&nbsp;<A HREF="#sec:Priorities-in-Negotiation-and-Preemption">3.4.3</A>.

<P>
One possible group quota policy is strict priority.
For example, a site prefers physics users to match as many slots as they can,
and only when all the physics jobs are running, 
and idle slots remain,
are chemistry jobs allowed to run.  
The default "starvation group order" can be used to implement this.  
By setting configuration variable 
<TT>NEGOTIATOR_ALLOW_QUOTA_OVERSUBSCRIPTION</TT><A NAME="31316"></A><A NAME="31317"></A> to <TT>True</TT>,
and setting the physics quota to a number so large that it cannot ever be met,
such as one million, 
the physics group will always be the "most starving" group,
will always negotiate first, 
and will always be unable to meet the quota.  
Only when all the physics jobs are running
will the chemistry jobs then run.  If the chemistry quota is set to a value smaller than physics, but still larger than the
pool, this policy can support a third, even lower priority group, and so on.

<P>
The <I>condor_userprio</I> command can show the current quotas in effect, 
and the current usage by group.
For example:
<PRE>
$ condor_userprio -quotas
Last Priority Update: 11/12 15:18
Group                          Effective  Config     Use    Subtree  Requested 
Name                             Quota     Quota   Surplus   Quota   Resources 
------------------------------ --------- --------- ------- --------- ----------
group_physics.hep                  15.00     15.00 no          15.00         60
group_physics.lep                   5.00      5.00 no           5.00         60
------------------------------ --------- --------- ------- --------- ----------
Number of users: 2                                 ByQuota
</PRE>

<P>
This shows that there are two groups, each with 60 jobs in the queue.  
group_physics.hep has a quota of 15 machines, 
and group_physics.lep has 5 machines.  
Other options to <I>condor_userprio</I>,
such as <B>-most</B> will also show the number of resources in use.

<HR>
<!--Navigation Panel-->
<A NAME="tex2html1493"
  HREF="3_5Policy_Configuration.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next" SRC="next.png"></A> 
<A NAME="tex2html1487"
  HREF="3_Administrators_Manual.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up.png"></A> 
<A NAME="tex2html1481"
  HREF="3_3Configuration.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="prev.png"></A> 
<A NAME="tex2html1489"
  HREF="Contents.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents" SRC="contents.png"></A> 
<A NAME="tex2html1491"
  HREF="Index.html">
<IMG WIDTH="43" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="index" SRC="index.png"></A> 
<BR>
<B> Next:</B> <A NAME="tex2html1494"
  HREF="3_5Policy_Configuration.html">3.5 Policy Configuration for</A>
<B> Up:</B> <A NAME="tex2html1488"
  HREF="3_Administrators_Manual.html">3. Administrators' Manual</A>
<B> Previous:</B> <A NAME="tex2html1482"
  HREF="3_3Configuration.html">3.3 Configuration</A>
 &nbsp; <B>  <A NAME="tex2html1490"
  HREF="Contents.html">Contents</A></B> 
 &nbsp; <B>  <A NAME="tex2html1492"
  HREF="Index.html">Index</A></B> 
<!--End of Navigation Panel-->

</BODY>
</HTML>
